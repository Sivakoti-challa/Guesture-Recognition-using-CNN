{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dell\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3068 images belonging to 6 classes.\n",
      "Found 641 images belonging to 6 classes.\n",
      "Epoch 1/50\n",
      "600/600 [==============================] - 148s 247ms/step - loss: 1.0727 - acc: 0.5817 - val_loss: 0.6636 - val_acc: 0.7600\n",
      "Epoch 2/50\n",
      "600/600 [==============================] - 133s 221ms/step - loss: 0.5704 - acc: 0.7903 - val_loss: 0.6618 - val_acc: 0.7000\n",
      "Epoch 3/50\n",
      "600/600 [==============================] - 131s 218ms/step - loss: 0.4058 - acc: 0.8600 - val_loss: 0.2358 - val_acc: 0.9067\n",
      "Epoch 4/50\n",
      "600/600 [==============================] - 116s 193ms/step - loss: 0.2868 - acc: 0.8983 - val_loss: 0.4468 - val_acc: 0.8467\n",
      "Epoch 5/50\n",
      "600/600 [==============================] - 116s 193ms/step - loss: 0.2540 - acc: 0.9120 - val_loss: 0.3254 - val_acc: 0.8973\n",
      "Epoch 6/50\n",
      "600/600 [==============================] - 123s 205ms/step - loss: 0.2203 - acc: 0.9277 - val_loss: 0.1797 - val_acc: 0.9267\n",
      "Epoch 7/50\n",
      "600/600 [==============================] - 114s 191ms/step - loss: 0.1871 - acc: 0.9397 - val_loss: 0.0954 - val_acc: 0.9800\n",
      "Epoch 8/50\n",
      "600/600 [==============================] - 119s 198ms/step - loss: 0.1716 - acc: 0.9428 - val_loss: 0.2150 - val_acc: 0.9467\n",
      "Epoch 9/50\n",
      "600/600 [==============================] - 116s 193ms/step - loss: 0.1379 - acc: 0.9543 - val_loss: 0.3778 - val_acc: 0.9178\n",
      "Epoch 10/50\n",
      "600/600 [==============================] - 114s 190ms/step - loss: 0.1383 - acc: 0.9583 - val_loss: 0.1977 - val_acc: 0.9533\n",
      "Epoch 11/50\n",
      "600/600 [==============================] - 115s 191ms/step - loss: 0.1114 - acc: 0.9630 - val_loss: 0.1321 - val_acc: 0.9733\n",
      "Epoch 12/50\n",
      "600/600 [==============================] - 115s 192ms/step - loss: 0.0893 - acc: 0.9707 - val_loss: 0.0697 - val_acc: 0.9733\n",
      "Epoch 13/50\n",
      "600/600 [==============================] - 115s 191ms/step - loss: 0.1013 - acc: 0.9687 - val_loss: 0.1311 - val_acc: 0.9795a\n",
      "Epoch 14/50\n",
      "600/600 [==============================] - 115s 192ms/step - loss: 0.0993 - acc: 0.9701 - val_loss: 0.1615 - val_acc: 0.9667\n",
      "Epoch 15/50\n",
      "600/600 [==============================] - 121s 202ms/step - loss: 0.0888 - acc: 0.9757 - val_loss: 0.1019 - val_acc: 0.9800\n",
      "Epoch 16/50\n",
      "600/600 [==============================] - 126s 210ms/step - loss: 0.0872 - acc: 0.9753 - val_loss: 0.0834 - val_acc: 0.9733\n",
      "Epoch 17/50\n",
      "600/600 [==============================] - 116s 194ms/step - loss: 0.0773 - acc: 0.9787 - val_loss: 0.1487 - val_acc: 0.9733\n",
      "Epoch 18/50\n",
      "600/600 [==============================] - 116s 194ms/step - loss: 0.0695 - acc: 0.9807 - val_loss: 0.1195 - val_acc: 0.9658\n",
      "Epoch 19/50\n",
      "600/600 [==============================] - 117s 194ms/step - loss: 0.0713 - acc: 0.9803 - val_loss: 0.1281 - val_acc: 0.9800\n",
      "Epoch 20/50\n",
      "600/600 [==============================] - 117s 196ms/step - loss: 0.0613 - acc: 0.9840 - val_loss: 0.1334 - val_acc: 0.9733\n",
      "Epoch 21/50\n",
      "600/600 [==============================] - 129s 215ms/step - loss: 0.0456 - acc: 0.9877 - val_loss: 0.1526 - val_acc: 0.9733\n",
      "Epoch 22/50\n",
      "600/600 [==============================] - 121s 202ms/step - loss: 0.0481 - acc: 0.9900 - val_loss: 0.2810 - val_acc: 0.9726\n",
      "Epoch 23/50\n",
      "600/600 [==============================] - 120s 199ms/step - loss: 0.0713 - acc: 0.9870 - val_loss: 0.1460 - val_acc: 0.9800\n",
      "Epoch 24/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0676 - acc: 0.9860 - val_loss: 0.1228 - val_acc: 0.9800\n",
      "Epoch 25/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0470 - acc: 0.9877 - val_loss: 0.1184 - val_acc: 0.9867\n",
      "Epoch 26/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0459 - acc: 0.9900 - val_loss: 0.1590 - val_acc: 0.9863\n",
      "Epoch 27/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0352 - acc: 0.9927 - val_loss: 0.0851 - val_acc: 0.9933\n",
      "Epoch 28/50\n",
      "600/600 [==============================] - 119s 198ms/step - loss: 0.0267 - acc: 0.9927 - val_loss: 0.1825 - val_acc: 0.9800\n",
      "Epoch 29/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0176 - acc: 0.9960 - val_loss: 0.1334 - val_acc: 0.9800\n",
      "Epoch 30/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0594 - acc: 0.9890 - val_loss: 0.1427 - val_acc: 0.9733\n",
      "Epoch 31/50\n",
      "600/600 [==============================] - 121s 201ms/step - loss: 0.0591 - acc: 0.9887 - val_loss: 0.0850 - val_acc: 0.9932\n",
      "Epoch 32/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0477 - acc: 0.9893 - val_loss: 0.2492 - val_acc: 0.9733\n",
      "Epoch 33/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0311 - acc: 0.9907 - val_loss: 0.1461 - val_acc: 0.9667\n",
      "Epoch 34/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0514 - acc: 0.9910 - val_loss: 8.9659e-05 - val_acc: 1.0000\n",
      "Epoch 35/50\n",
      "600/600 [==============================] - 119s 198ms/step - loss: 0.0309 - acc: 0.9923 - val_loss: 0.2337 - val_acc: 0.9726\n",
      "Epoch 36/50\n",
      "600/600 [==============================] - 119s 198ms/step - loss: 0.0546 - acc: 0.9900 - val_loss: 0.0894 - val_acc: 0.9933\n",
      "Epoch 37/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0324 - acc: 0.9940 - val_loss: 0.1360 - val_acc: 0.9867\n",
      "Epoch 38/50\n",
      "600/600 [==============================] - 119s 198ms/step - loss: 0.0236 - acc: 0.9930 - val_loss: 2.9053e-04 - val_acc: 1.0000\n",
      "Epoch 39/50\n",
      "600/600 [==============================] - 120s 199ms/step - loss: 0.0402 - acc: 0.9930 - val_loss: 0.1743 - val_acc: 0.9726\n",
      "Epoch 40/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0493 - acc: 0.9903 - val_loss: 0.0337 - val_acc: 0.9867\n",
      "Epoch 41/50\n",
      "600/600 [==============================] - 121s 201ms/step - loss: 0.0270 - acc: 0.9927 - val_loss: 0.1419 - val_acc: 0.9800\n",
      "Epoch 42/50\n",
      "600/600 [==============================] - 139s 231ms/step - loss: 0.0443 - acc: 0.9937 - val_loss: 0.2751 - val_acc: 0.9667\n",
      "Epoch 43/50\n",
      "600/600 [==============================] - 135s 225ms/step - loss: 0.0446 - acc: 0.9907 - val_loss: 0.0222 - val_acc: 0.9863\n",
      "Epoch 44/50\n",
      "600/600 [==============================] - 163s 271ms/step - loss: 0.0406 - acc: 0.9937 - val_loss: 0.1731 - val_acc: 0.9800\n",
      "Epoch 45/50\n",
      "600/600 [==============================] - 124s 206ms/step - loss: 0.0192 - acc: 0.9958 - val_loss: 0.1887 - val_acc: 0.9867\n",
      "Epoch 46/50\n",
      "600/600 [==============================] - 119s 199ms/step - loss: 0.0352 - acc: 0.9940 - val_loss: 0.1805 - val_acc: 0.9867\n",
      "Epoch 47/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0429 - acc: 0.9940 - val_loss: 0.0010 - val_acc: 1.0000\n",
      "Epoch 48/50\n",
      "600/600 [==============================] - 120s 200ms/step - loss: 0.0371 - acc: 0.9950 - val_loss: 0.0792 - val_acc: 0.9932\n",
      "Epoch 49/50\n",
      "600/600 [==============================] - 120s 199ms/step - loss: 0.0157 - acc: 0.9973 - val_loss: 0.1072 - val_acc: 0.9933\n",
      "Epoch 50/50\n",
      "467/600 [======================>.......] - ETA: 26s - loss: 0.0457 - acc: 0.9927"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Convolution2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers.convolutional_recurrent import ConvLSTM2D\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Lambda\n",
    "from keras import optimizers\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "img_width, img_height =  64,64\n",
    "num_classes = 6\n",
    "nb_train_samples = num_classes*70\n",
    "nb_validation_samples = num_classes*20\n",
    "epochs = 10\n",
    "batch_size = 5\n",
    "input_shape = (img_width, img_height)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Lambda(lambda x: x[:,:,:,0], input_shape=(*input_shape, 3)))\n",
    "model.add(LSTM(units=256, input_shape= input_shape, return_sequences=True))\n",
    "model.add(LSTM(units=128, return_sequences=True))\n",
    "model.add(LSTM(units=32))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "test_datagen = ImageDataGenerator(rescale = 1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'data/train',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    'data/test',\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=600,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=30)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"model-bw1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights('model-bw1.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import operator\n",
    "import cv2\n",
    "import sys, os\n",
    "\n",
    "# Loading the model\n",
    "json_file = open(\"model-bw1.json\", \"r\")\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model-bw1.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Category dictionary\n",
    "categories = {0: 'LIGHTS OFF', 1: 'LIGHTS ON', 2: 'FAN ON', 3: 'FAN OFF', 4: 'CHARGING ON', 5: 'CHARGING OFF'}\n",
    "\n",
    "while True:\n",
    "    _, frame = cap.read()\n",
    "    # Simulating mirror image\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Got this from collect-data.py\n",
    "    # Coordinates of the ROI\n",
    "    x1 = int(0.5*frame.shape[1])\n",
    "    y1 = 10\n",
    "    x2 = frame.shape[1]-10\n",
    "    y2 = int(0.5*frame.shape[1])\n",
    "    # Drawing the ROI\n",
    "    # The increment/decrement by 1 is to compensate for the bounding box\n",
    "    cv2.rectangle(frame, (x1-1, y1-1), (x2+1, y2+1), (255,0,0) ,1)\n",
    "    # Extracting the ROI\n",
    "    roi = frame[y1:y2, x1:x2]\n",
    "    \n",
    "    # Resizing the ROI so it can be fed to the model for prediction\n",
    "    roi = cv2.resize(roi, (64, 64)) \n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "    _, test_image = cv2.threshold(roi, 120, 255, cv2.THRESH_BINARY)\n",
    "    cv2.imshow(\"test\", test_image)\n",
    "    # Batch of 1\n",
    "    result = loaded_model.predict(test_image.reshape(1, 64, 64, 3))\n",
    "    prediction = {'LIGHTS OFF': result[0][0], \n",
    "                  'LIGHTS ON': result[0][1], \n",
    "                  'FAN ON': result[0][2],\n",
    "                  'FAN OFF': result[0][3],\n",
    "                  'CHARGING ON': result[0][4],\n",
    "                  'CHARGING OFF': result[0][5]}\n",
    "    # Sorting based on top prediction\n",
    "    prediction = sorted(prediction.items(), key=operator.itemgetter(1), reverse=True)\n",
    "    \n",
    "    # Displaying the predictions\n",
    "    cv2.putText(frame, prediction[0][0], (10, 120), cv2.FONT_HERSHEY_PLAIN, 1, (255,0,255), 1)    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    interrupt = cv2.waitKey(10)\n",
    "    if interrupt & 0xFF == 27: # esc key\n",
    "        break\n",
    "        \n",
    " \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
